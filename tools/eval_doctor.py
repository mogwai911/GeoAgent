import json
import re
from typing import List, Dict, Annotated
from pydantic import BaseModel, Field
from langchain_core.tools import tool, InjectedToolCallId
from langgraph.prebuilt import InjectedState
from langchain_core.messages.ai import AIMessage
from langchain_openai import ChatOpenAI
from sentence_transformers import SentenceTransformer, util
from config.model_config import get_model_config
from prompts.tool_prompt_templates import EVAL_DOCTOR_TEMPLATE

# === Load LLM & Embedding Model ===
model_config = get_model_config()

llm = ChatOpenAI(
    base_url=model_config['base_url'],
    api_key=model_config['api_key'],
    model_name=model_config['model_name'],
    streaming=True)

embedding_model = SentenceTransformer("nomic-ai/nomic-embed-text-v2-moe", trust_remote_code=True)

# === Input Schema ===
class FileMetadata(BaseModel):
    path: str
    type: str
    geometry: str = ""
    fields: List[str]
    features: int
    crs: str = "Unknown"
    description: str

class EvalInput(BaseModel):
    query: str = Field(..., description="User's original task description")
    context: List[Dict[str, str]] = Field(..., description="RAG returned context in structured form")
    file_metadata: FileMetadata = Field(..., description="Metadata of the file selected for the task")
    generated_code: str = Field(..., description="The final code generated by the code agent")

# === Output Schema ===
class EvalScoreExplanation(BaseModel):
    score: float
    reasoning: str

class EvalOutput(BaseModel):
    context_relevance: EvalScoreExplanation
    context_coverage: EvalScoreExplanation
    code_accuracy: EvalScoreExplanation
    code_context_consistency: EvalScoreExplanation
    similarity: Dict[str, float]


# === Embedding Similarity Helper ===
def cosine_sim(a: str, b: str) -> float: 
    emb = embedding_model.encode([a, b], convert_to_tensor=True) 
    return float(util.pytorch_cos_sim(emb[0], emb[1]))

# === Tool Implementation ===
@tool(
    "eval_doctor", 
    args_schema=EvalInput, 
    return_direct=False, 
    description="Evaluate whether the code and retrieved context help solve the user task using LLM analysis and semantic similarity." ) 
def eval_doctor(
    query: str,
    context: List[Dict[str, str]],
    path: str,
    type: str,
    # state: Annotated[dict, InjectedState], 
    # call_id: Annotated[str, InjectedToolCallId],
    geometry: str = "",
    features: int = 0,
    fields: List[str] = [], 
    crs: str = "Unknown",
    description: str = "",
    generated_code: str = "" ) -> EvalOutput: 
    #print(f"ðŸ§ª [eval_doctor] Evaluating result quality... Call ID: {call_id}")

    file_metadata = FileMetadata(
        path=path,
        type=type,
        geometry=geometry,
        fields=fields,
        features=features,
        crs=crs,
        description=description,
    )

    input = EvalInput(
        query=query,
        context=context,
        file_metadata=file_metadata,
        generated_code=generated_code,
    )

    context_str = "\n\n".join([f"# Source: {c['source']}\n{c['content']}" for c in input.context])
    # --- Step 1: LLM-Based Evaluation ---
    prompt = EVAL_DOCTOR_TEMPLATE.render(
        query=input.query,
        context=context_str,
        file_metadata=input.file_metadata,
        generated_code = input.generated_code
    )

    raw_output = llm.invoke(prompt)
    if isinstance(raw_output, AIMessage):
        raw_output = raw_output.content or ""
    elif not isinstance(raw_output, str):
        raw_output = str(raw_output or "")

    try:
        llm_eval = json.loads(re.search(r"\{.*\}", raw_output, re.DOTALL).group(0))
    except Exception:
        llm_eval = {
            "context_relevance": {
                "score": 0.0,
                "reasoning": "Failed to parse evaluation."
            },
            "context_coverage": {
                "score": 0.0,
                "reasoning": "Failed to parse evaluation."
            },
            "code_accuracy": {
                "score": 0.0,
                "reasoning": "Failed to parse evaluation."
            },
            "code_context_consistency": {
                "score": 0.0,
                "reasoning": "Failed to parse evaluation."
            }
        }

    context_text = " ".join([c["content"] for c in input.context])
    full_task_text = "\n".join([
        f"Query: {input.query}",
        f"File Metadata: {input.file_metadata.json()}",
        f"RAG Context: {context_text}"
    ])

    similarity = {
        "context_to_code": cosine_sim(context_text, input.generated_code),
        "full_task_to_code": cosine_sim(full_task_text, input.generated_code)
    }

    return EvalOutput(
    context_relevance=EvalScoreExplanation(**llm_eval["context_relevance"]),
    context_coverage=EvalScoreExplanation(**llm_eval["context_coverage"]),
    code_accuracy=EvalScoreExplanation(**llm_eval["code_accuracy"]),
    code_context_consistency=EvalScoreExplanation(**llm_eval["code_context_consistency"]),
    similarity=similarity
    )

